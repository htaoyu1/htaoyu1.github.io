---
layout: post
title: 数据挖掘之关联规则分析（4）：关联规则的产生
category: Data Mining 
author: Hongtao Yu
tags: 
  - data-mining 
  - association-analysis
comments: true
use_math: true
lang: zh
---

- TOC
{:toc}



# 引言

在[`Apriori 算法`](http://localhost:4000/data%20mining/2017/12/18/Association-Rule-2/)和[`FP-Growth 算法`](http://localhost:4000/data%20mining/2017/12/24/Association-Rule-3/)中，我们介绍了如何从数据集中产生频繁项集（频繁模式）。这里我们谈一下如果从挖掘到的频繁模式中产生关联规则。

# 支持度计数

在之前的讨论中我们知道，一个强关联规则不但要满足最小支持度，还要满足最小置信度。置信度度量了关联规则的准确度。对于关联规则 $X \Rightarrow Y$, 其置信度定义为:

$$
\text{Confidence}(X \Rightarrow Y) = P(Y \vert X) = \frac{\sigma(X \cup Y)}{\sigma(X)}
$$

其中 $\sigma(\cdot)$ 为支持度计数函数，即事务集中有多少条事务包含了项集 $X$。置信度的涵义为，在出现 $X$ 的事务中，有多大的概率同时包含了 $Y$。对于事务集 $T$ 中的每一条事务 $t_i$, 我们需要 $t_i$ 包含的所有子项集 $X$，如果 $X$ 包含在频繁项集 $L$ 中，那么将 $L$ 中 $X$ 的支持度计数增加1。

## 枚举子项集

支持度计数的第一步是枚举一条事务 $t_i$ 中包含的所有子项集。枚举的标准有两个（1）不遗漏 $t_i$ 中包含的任何子项集；（2）不重复产生子项集。下面以一个例子说明如何系统地枚举一个事务中包含的子集。假定我们有一个事务 $t$ = {2, 3, 6, 5, 1}，需要找到其包含的所有 3-项集。



为了系统地枚举，我们需要首先对 $t$ 中包含的项从小到大排序，得到 $t$ = {1, 2, 3, 5, 6}。我们产生子集的时候，保证每个子集中的项都是从小到大排列。即我们产生的 3-项集都是以 1, 2, 3开头，而不必产生以 5, 6 开头的项集。下图的前缀结构描述了这种系统的方法。

![4-1 枚举事务 $t$ 中包含的所有 3-项集](/assets/blog-images/Association-Rule-4.1.png)
**Fig. 4-1.** 枚举事务 $t$ 中包含的所有 3-项集（图片来自于参考文献 1）。

**Fig. 4-1** 中的第一层，我们首先确定每个 3-项集的第一个项，后面的两个项可以从 $t$ 中剩余的项提取。因为我们要保证产生的每个 3-项集中的项都是按增序排列，所以提取第一个项后，不必再考虑 $t$ 中比第一项小的项。例如我们提取第一个项为 2, 那么 $t$ 中剩余的项我们只需考虑 {3, 5, 6}，而不必再考虑项 1。因为我们不会产生 {2 ,1, 3} 这样的 3-项集。事实上，{2, 1, 3} 这个 3-项集 等价于 {1, 2, 3}, 而后者将会在以 1 开头的3-项集中产生。最终，我们得到了三个子类。对于每个子类，我们接着提取第二个项，得到第二层。然后从剩余的项中选取 3-项集的最后一个项。得到了 $t$ 中包含的所有 10 个 3-项集（**Fig. 4-1** 的第三层）。

找到 $t$ 中包含的所有 3-项集后，接下来我们需要确定每个 3-项集是否包含在频繁 3-项集 $L_3$ 中。

## 使用 Hash 树更新支持度计数

为了查找事务 $t$ 中的一个子项集是否包含在频繁项集 $L$ 中，蛮力方法需要将该子项集与$L$ 中的每一个项集进行比较。这就造成很大的计算开销。为了减小计算量，我们可以使用 `Hash 函数`将频繁项集 $L$ 划分为不同的桶，并存放在 `Hash 树`中。在支持度计数统计时，$t$ 中的每一个子集也被散列到相应的桶中，并且只与桶中的频繁项集比较。


对于一个项 $X$, 假如我们定义哈希函数 $h(X) = X \mod 3$，那么我们可以使用每个内部节点包含三个分支的 Hash 树对项集进行散列：

$$
h(X) = \left\{
  \begin{array}{ll}
    1       & X\ 被散列到节点的左分枝，\\ 
    2       & X\ 被散列到节点的中间分枝，\\
    0       & X\ 被散列到节点的右分枝。
  \end{array}
\right.
$$

![4-2 构建频繁项集的 Hash 树](/assets/blog-images/Association-Rule-4.2.png)
**Fig. 4-2.** 构建频繁项集的 Hash 树。

**Fig. 4-2** 展示了如果如何对一个频繁项集构建对应的 Hash 树。考虑图中所示频繁  3-项集 $L_3$，假设树上叶子节点的最大项集数为3。我们首先取出 $L_3$ 中的第一个项集 $L_3^{(1)} $ = {1,4,5}，由于 $L_3^{(1)}$ 中的第一个项 $L_3^{(1)}[1] = 1$, 并且 $ 1\mod 3 = 1$, 所以 $L_3^{(1)} $ 被映射到 Hash 树的左节点。接着我们取出第二个项集 $L_3^{(2)} $ = {1,2,4}，$L_3^{(2)}[1] \mod 3 = 1$, 因此 $L_3^{(2)} $ 同样被映射到左节点。同理，$L_3^{(3)} $ 和 $L_3^{(4)} $ 也被映射到树的左节点。但是由于第四项插入时树的叶子节点个数超过了 3， 因此我们需要进行分裂操作。把该节点转换为树的内部节点，然后使用这四个项集的第二个项做 Hash 操作，将他们分别映射到第二层对应的节点。类似地，由于 $L_3^{(5)}$ 的插入导致另一个叶子节点包含的项集个数超过了 3，我们使用项集的第 3 个项将他们分别映射到树的第三层对应的叶子节点上。这样依次对每个项集进行操作，我们得到了频繁项集 $L_3$ 对应的 Hash 树。


![4-3 更新频繁项集的支持度计数](/assets/blog-images/Association-Rule-4.3.png)
**Fig. 4-3.** 更新频繁项集的支持度计数。

有了频繁项集的 Hash 树以后，我们就可以根据事务更新频繁项集的支持度计数了。仍然以 **Fig. 4-1** 中的事务 $t$ = {1， 2，3， 5，6} 为例。根据 **Fig. 4-1** 的第一层前缀结构，我们分别将以 1， 2， 和 3 开头的项集散列到 Hash 树的左，中，右节点。如果一个节点是内部节点，我们根据第二项的 Hash 值将前缀结构继续散列，直到到达一个叶子节点。然后使用这个前缀结构产生事务 $t$ 中包含的子项集，并与 Hash 树的叶子节点包含的频繁项集进行比较，将匹配的频繁项集的计数增加 1（**Fig. 4-3** 红色的项集）。


# 产生关联规则

有了频繁项集以及各频繁项集的支持度计算，我们就可以挖掘数据集中的强规则了。考虑一个频繁 3-项集 $Y$ = {A, B, C}, 可以产生 $2^3 - 2 = 6$ 个规则：{A, B} $\Rightarrow$ {C}, {A, C} $\Rightarrow$ {B}, {B, C} $\Rightarrow$ {A}, {A} $\Rightarrow$ {B, C}, {B} $\Rightarrow$ {A, C} 以及 {C} $\Rightarrow$ {A, B}。他们的支持度都等于 $Y$ 的支持度，但是置信度不一样。支持度具有反单调的性质，但是置信度不具有任何单调性，即由 $\widetilde{X} \subseteq X$ 且 $\widetilde{Y} \subseteq Y$, 并不能推导出 $\text{Confidence} (\widetilde{X} \Rightarrow \widetilde{Y}) \geq \text{Confidence} (X \Rightarrow Y)$ 或则 $\text{Confidence} (\widetilde{X} \Rightarrow \widetilde{Y}) \leq \text{Confidence} (X \Rightarrow Y)$。

从上面的例子我们可以看出，对于一个频繁项集，关联规则可以通过公式 $X \Rightarrow Y-X$ 产生，即将频繁项集一分为二，一部分做先导，一部分做后继。这样我们可以使用逐层方法来产生关联规则。首先提取频繁项集的一项作为后继，找到所有关联规则；然后合并两个规则的后继，产生后继包含两个项的新规则，以此类推。考虑频繁项集 $Y$ = {A, B, C, D}，我们可以首先提取其中任意一个项做后继，产生四个规则 {A, B, C} $\Rightarrow$ {D}, {A, B, D} $\Rightarrow$ {C}, {A, C, D} $\Rightarrow$ {B}, 和 {B, C， D} $\Rightarrow$ {A}。然后合并任意两个规则，产生新的规则。比如合并 {A, B, C} $\Rightarrow$ {D} 和 {A, B, D} $\Rightarrow$ {C} 我们得到规则 {A, B} $\Rightarrow$ {C, D}。**Fig. 4-4** 展示了这样一个过程。

![4-4 规则的产生以及基于置信度的剪枝](/assets/blog-images/Association-Rule-4.4.png)
**Fig. 4-4.** 规则的产生以及基于置信度的剪枝（图片来自于参考文献 3）。

虽然规则的置信度不具有任何单调性，但是我们仍然可以基于下面的定理剪枝：

**定理**：如果规则 $X \Rightarrow Y-X$ 小于置信度阈值，那么 $X‘ \Rightarrow Y-X’$ 也一定不满足置信度阈值，其中 $X' \subseteq X$。

> 证明：因为 $X' \subseteq X$, 根据支持度的反单调性，我们有 $\sigma(X') \geq \sigma(X)$，因此
> 
> $$
> \text{Confidence}(X' \Rightarrow Y-X') = \frac{\sigma(Y)}{\sigma(X')} \leq \frac{\sigma(Y)}{\sigma(X)} = \text{Confidence}(X \Rightarrow Y-X)
> $$

 因此，如果一个规则低于最小置信度，我们接下来不用再考虑用它与其他规则组合产生新的规则。比如在 **Fig. 4-4** 中，如果规则 {B, C, D} $\Rightarrow$ {A} 不满足最小置信度，那么我们可以直接剪除后继包含 {A} 的所有规则。

# References


1. [数据挖掘导论](https://book.douban.com/subject/5377669/)，P.-N. Tan, Michael Steinbach, and V. Kumar 著；范明，范宏建 译， 人民邮电出版社, **2006.**

2. [Apriori导论](http://www.voidcn.com/article/p-whbfxrod-vp.html)


3. [Data Mining Lecture 3](http://www.cs.uoi.gr/~tsap/teaching/2012f-cs059/material/datamining-lect3.pdf)






