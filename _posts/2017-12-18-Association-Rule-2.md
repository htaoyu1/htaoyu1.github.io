---
layout: post
title: 数据挖掘之关联规则分析（2）：Apriori 算法
category: Data Mining 
author: Hongtao Yu
tags: 
  - data-mining 
  - association-analysis
comments: true
use_math: true
lang: zh
---

- TOC
{:toc}



# 引言


关联规则（Association Rules）分析的目的就是挖掘出数据集中感兴趣的关系。即给定事务集 $T$, 找出其中支持度 $\geq$ min_sup 并且置信度 $\geq$ min_conf 的所有规则。其中 min_sup 和 min_conf 为预定义的阈值。`Apriori 算法`是一种常用的关联规则挖掘算法，它主要的目的是用来找出数据集中频繁出现的项集。考虑一个包含 $d$ 个项的项集，如果使用暴力解法，穷举出其中所有的子项集, 那么最终的结果会是指数级的 $\sum_{i=0}^d C_d^i = 2^d$, 而其所能形成的关联规则的总数则是 $3^d - 2^{d+1} + 1$。

> 证明：
> 
>   假设先导为 $k$-项集，那么可以形成的总的组合个数为 $C_d^k$, 后继可以是从剩余的 $d-k$个项目里选取的任意 $j = 1, 2, \cdots, d-k$ 个项目。因此对于先导为 $k$-项集的规则的总的个数为 $C_d^k \times \sum\limits_{j=1}^{d-k} C_{d-k}^j$。考虑到 $k$ 可以是从 $1$ 到 $d-1$ 之间的所有数值，从而得到包含 $d$ 个项目的项集可以形成的规则总数是：
>   
$$
R = \sum\limits_{k=1}^{d-1}\left[ C_d^k \times \sum\limits_{j=1}^{d-k} C_{d-k}^j\right] = 3^d - 2^{d+1} + 1
$$

我们可以看到，这个数目是巨大的。仅仅对于 $I$ = {牛奶,面包,尿布,啤酒,鸡蛋,可乐} 这么简单的情况，就需要计算 $3^6 - 2^7 + 1 = 602$ 条规则。然而现实中，这些规则大部分在数据集中都不会出现，或者具有很小的支持度和置信度，没有挖掘的价值，因此使得大部分计算都是无用。为了减小计算的开销，我们可以先对规则进行剪枝，剪除一些支持度和置信度低的规则，只关注在数据集中频繁出现的规则。

很直观地，如果一个规则是频繁的，那么它包含的项目集也应该是频繁的。基于这个思路，我们可以把关联规则挖掘分解为两个子任务:

1. **频繁项集产生**：目标是发现满足最小支持度阈值的所有项集，这些项集被称为`频繁项集`（frequent itemset）。通常这一步需要非常大的计算开销。

2. **规则的产生**: 目标是从步骤1发现的频繁项集中提取所有高置信度的规则，这些规则被称为`强规则`（strong rule）。这一步的计算开销远小于步骤1。


# 先验（Apriori）原理

Apriori 算法由 Agrawal 和 Srikant 于 1994 年率先提出来。该算法是基于下面的`先验原理`:

- **先验原理**: 如果一个项集是频繁的，那么它的所有子集也一定是频繁的。

以下面的超市购物篮数据集为例：

| TID   |      Items       |
|:-----:|:----------------:|
| $t_1$	| 牛奶,面包          |
| $t_2$	| 面包,尿布,啤酒,鸡蛋 |
| $t_3$ | 牛奶,尿布,啤酒,可乐 |
| $t_4$ | 面包,牛奶,尿布,啤酒 |
| $t_5$ | 面包,牛奶,尿布,可乐 |

我们发现其中的项集 {牛奶，尿布} 是频繁的项集，5条事务中出现了3次（$t_3, t_4, t_5$）, 因此其子项集 {牛奶}（出现4次: $t_1, t_3, t_4, t_5 $）和 {尿布} 也都是频繁的（出现4次: $t_2, t_3, t_4, t_5 $）。

![2-1 Apriori 原理与基于支持度的剪枝](/assets/blog-images/Association-Rule-2.1.png)
**Fig. 2-1:** Apriori 原理原理与基于支持度的剪枝。如果 {c, d, e} 是频繁的，那么它所有的子集也一定是频繁的。如果 {a, b} 是非频繁的，那么它所有的超集也一定是非频繁的，可以立即剪去。

相反地，如果一个项集是非频繁的，那么它所有的超集也一定是非频繁的（**Fig 2-1**）。因此我们一旦发现一个项集的支持度低于我们的阈值，那么我们可以直接剪掉该项集以及其所有的超集。这种基于支持度度量修剪指数搜索空间的策略被称为**基于支持度的剪枝**（support-based pruning）。这种策略依赖于支持度度量的一个关键性质，即一个项集的支持度绝不会超过它的子集的支持度。这个性质也称为支持度度量的`反单调性`（anti-monotone）。


# 支持度的单调性与反单调性

令 $I$ 是项的集合，$J = 2^{\vert I \vert}$ 是 $I$ 的幂集。如果有 

$$
\forall X, Y \in J: (X \subseteq Y) \to f(X) \leq f(Y)
$$

则称度量 $f$ 是`单调的`（或向上封闭的）。这里的意思是，如果 $X$ 是 $Y$ 的一个子集，那么 $X$ 的度量 $f(X)$ 一定不会超过 $f(Y)$。例如如果我们定义 $f = \vert \cdot \vert$ 为计算集合 $X$ 中包含元素个数的函数，那么有 $\vert X \vert \leq \vert Y \vert$。

相反地，如果度量 $f$ 满足：

$$
\forall X, Y \in J: (X \subseteq Y) \to f(X) \geq f(Y)
$$

即如果 $X$ 是 $Y$ 的一个子集，那么 $Y$ 的度量 $f(Y)$ 一定不会超过 $f(X)$。则称度量 $f$ 为`反单调的`（或向下封闭的）。例如我们这里的支持度度量。

任何具有反单调性的度量都能够直接结合到挖掘算法中对候选项集的指数搜索空间进行有效地剪枝。

# Apriori 算法

从名字可以看出，Apriori 算法基于频繁项集的先验原理，使用一种称为逐层搜索的迭代方法，使用 $k$-频繁项集搜索 $(k+1)$ 项集。具体做法为，扫描数据库，计算每个 $1$-项集的支持度，删除所有支持度小于预定义阈值的项集，找出频繁 $1$-项集，记为 $L_1$。基于 $L_1$ 产生候选 $2$-项集 $C_2$，然后再次扫描数据库，寻找 $C_2$ 中满足最小支持的频繁 $2-$项集，组成集合 $L_2$。然后依此迭代，找到 $L_3$, ... 直到不能再找到频繁 $k+1$-项集。对应的频繁 $k$-项集即为算法的输出结果。流程如下：

$$
L_1 \to C_2 \to L_2 \to C_3 \to L_3 \to \cdots \to L_k
$$

Apriori 算法寻找每个 $L_i$ 项集需要进行一次完整的数据库扫描。

![2-2 Apriori 算法的流程](/assets/blog-images/Association-Rule-2.2.png)
**Fig. 2-2:** Apriori 算法的流程。为了更好地说明从 $L_2$ 到 $C_3$ 的连接操作，这里我把 $L_2$ 里的项按其对应拼音的顺序进行了排列。很多教材是按照这几个单词对应的英文排列的，因此只产生了包含一个元素的 $C_3$ 候选集。

> 注：我拼了老命也没看出来 {面包，尿布，牛奶} 的计数为嘛是3。但是我看的一些 Blog，Slides，甚至包括《数据挖掘导论》这本书在内，都说它的计数为3。

**Fig. 2-2** 显示了 Apriori 算法的具体操作流程。基于我们的购物篮数据，首先扫描数据库，产生了包含6个元素的候选 $1-$项集 $C_1$。然后删除不满足最小支持度的项集，得到包含4个元素的频繁 $1$-项集 $L_1$。使用 $L_1$ 产生包含 $C_4^2 = 6$ 个元素的候选 $2$-项集 $C_2$。再次扫描数据库，计算每个 $2$-项集的支持度，删除不满足最小支持度的项集，得到包含4个元素的频繁 $2$-项集 $L_2$。这里值得一提的是如何从 $L_2$ 产生候选 $3$-项集 $C_3$。如果不使用 Apriori 算法，我们看到 $L_2$ 包含了4个项 {牛奶，面包，尿布，啤酒}。蛮力算法将会给出包含 $C_4^3 = 4$ 个元素的候选 $3$-项集 $C_3$。Apriori 使用了一种被称为`连接`的操作来压缩搜索空间。

**连接**： 为了从频繁 $k-1$-项集 $L_{k-1}$ 产生候选 $C_k$ 项集，Apriori 算法首先将项集中的项按字典顺序排列。我们记 $L_{k-1}$ 项集的第 $i$ 个元素为 $L_{k-1}^{(i)}$，将 $L_{k-1}^{(i)}$ 中的第 $m$ 项记为 $L_{k-1}^{(i)}[m]$。使用字典排序意味着 $L_{k-1}^{(i)}$ 中的每一项有 $L_{k-1}^{(i)}[1] < L_{k-1}^{(i)}[2] < \cdots < L_{k-1}^{(i)}[k-1]$。如果 $L_{k-1}^{(i)}$ 和 $L_{k-1}^{(j)}$ 只有最后一项不同，即 

$$
\begin{eqnarray*}
L_{k-1}^{(i)}[1] & = & L_{k-1}^{(j)}[1] , \\
L_{k-1}^{(i)}[2] & = & L_{k-1}^{(j)}[2] , \\
\cdots & & , \\ 
L_{k-1}^{(i)}[k-2] & = & L_{k-1}^{(j)}[k-2], \\
L_{k-1}^{(i)}[k-1] & < & L_{k-1}^{(j)}[k-1]
\end{eqnarray*}
$$

那么称 $L_{k-1}^{(i)}$ 和 $L_{k-1}^{(j)}$ 是可连接的。连接操作将产生一个 $k$-项集

$$
L_{k-1}^{(i)} \bowtie L_{k-1}^{(j)}  = \{ L_{k-1}^{(i)}[1], L_{k-1}^{(i)}[2], \cdots L_{k-1}^{(i)}[k-1], L_{k-1}^{(j)}[k-1] \} 
$$

注意到上面的操作中我们使用了约束 $L_{k-1}^{(i)}[k-1]  <  L_{k-1}^{(j)}[k-1]$，这是为了确保连接的过程中不产生重复的项集。


以 **Fig. 2-2** 中的 $L_2$ 项集为例，经过排序（这里我们按拼音字母排序）操作，我们得到包含 4 个元素的 $2$-项集： $L_2^{(1)} = $ {面包，牛奶}，$L_2^{(2)} = $ {尿布，牛奶}，$L_2^{(3)} = $ {面包，尿布}，$L_2^{(4)} = $ {尿布，啤酒}。可以看出，这里能进行连接操作的为 $L_2^{(1)} \bowtie L_2^{(3)} = $ {面包，尿布，牛奶} 和 $L_2^{(2)} \bowtie L_2^{(4)} = $ {尿布，牛奶，啤酒}。因此我们得到了包含2个元素的 $C_3$ 集合。对 $C_3$ 进行剪切，删除不满足最小支持度的项集，我们得到了 $L_3$。由于 $L_3$ 至包含一个元素，没有办法生成更高一级的候选项集，算法结束。



  

# References

1. R. Agrawal and R. Srikant, "[Fast Algorithms for Mining Association Rules in Large Databases,](http://www.vldb.org/conf/1994/P487.PDF)" in *Proceedings of the 20th International Conference on Very Large Data Bases,* **1994,** pp. 487-499.


2. [数据挖掘导论](https://book.douban.com/subject/5377669/)，P.-N. Tan, Michael Steinbach, and V. Kumar 著；范明，范宏建 译， 人民邮电出版社, **2006.**

3. [数据挖掘：概念与技术](https://book.douban.com/subject/2038599/)，Jiawei Han, Micheline Kamber, and Jian Pei 著；范明，孟小峰 译， 机械工业出版社, **2007.**

4. [Apriori 算法原理总结](http://www.cnblogs.com/pinard/p/6293298.html)



